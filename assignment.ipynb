{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"emotion_dataset.txt\"\n",
    "\n",
    "# Read the file\n",
    "with open(filename, 'r') as file:\n",
    "    lines = file.readlines()  \n",
    "\n",
    "# Each line as a json dictionary\n",
    "dataset = [json.loads(line) for line in lines]\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"\\nMissing values per colummn:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution\n",
    "class_distribution = df['label'].value_counts().sort_index()\n",
    "\n",
    "class_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of the Emotion variable')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Number of examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text lengths\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "\n",
    "print(\"\\nStadistics:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "plt.hist(df['text_length'], bins=30, color='lightgreen', edgecolor='black')\n",
    "plt.title('Text length distribution')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "#temporal variable to divide in test and validation\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "#test and validation\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=123)\n",
    "\n",
    "X_train_half, X_temp_half, y_train_half, y_temp_half = train_test_split(X[:int(X.shape[0]/2)], y[:int(y.shape[0]/2)], test_size=0.3, random_state=123)\n",
    "X_test_half, X_val_half, y_test_half, y_val_half = train_test_split(X_temp_half, y_temp_half, test_size=0.5, random_state=123)\n",
    "\n",
    "# Sizings\n",
    "print(f\"Training data size: {len(X_train)}\")\n",
    "print(f\"Validation data size: {len(X_val)}\")\n",
    "print(f\"Test data size: {len(X_test)}\")\n",
    "\n",
    "print(f\"Training data size: {len(X_train_half)}\")\n",
    "print(f\"Validation data size: {len(X_val_half)}\")\n",
    "print(f\"Test data size: {len(X_test_half)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer TF-IDF. Text format\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_half = vectorizer.fit_transform(X_train_half)\n",
    "X_val_half = vectorizer.transform(X_val_half)\n",
    "X_test_half = vectorizer.transform(X_test_half)\n",
    "\n",
    "print(\"Training data TF-IDF: \")\n",
    "print(X_train)\n",
    "print(f\"Validation data TF-IDF: {X_val}\")\n",
    "print(f\"Test data TF-IDF: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels format\n",
    "print(f\"Tipo de y_train: {type(y_train.iloc[0])}\")\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "y_train_half = y_train_half.astype(int)\n",
    "y_val_half = y_val_half.astype(int)\n",
    "y_test_half = y_test_half.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority classifier\n",
    "predicted_labels = [y_train.value_counts().idxmax()]*len(y_test)\n",
    "print(\"Majority classifier Accuracy:\")\n",
    "MC_accuracy_score = accuracy_score(y_test, predicted_labels)\n",
    "print(MC_accuracy_score)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_labels))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority classifier with half the dataset\n",
    "predicted_labels = [y_train_half.value_counts().idxmax()]*len(y_test_half)\n",
    "print(\"Majority classifier Accuracy:\")\n",
    "MC_accuracy_score_half = accuracy_score(y_test_half, predicted_labels)\n",
    "print(MC_accuracy_score_half)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_half, predicted_labels))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test_half, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "predicted_knn = knn.predict(X_test)\n",
    "print(\"KNN Accuracy:\")\n",
    "KNN_accuracy_score = accuracy_score(y_test, predicted_knn)\n",
    "print(KNN_accuracy_score)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_knn))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with half the dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_half, y_train_half)\n",
    "predicted_knn = knn.predict(X_test_half)\n",
    "print(\"KNN Accuracy:\")\n",
    "KNN_accuracy_score_half = accuracy_score(y_test_half, predicted_knn)\n",
    "print(KNN_accuracy_score_half)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_half, predicted_knn))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test_half, predicted_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "predicted_svm = svm.predict(X_test)\n",
    "print(\"SVM Accuracy:\")\n",
    "SVM_accuracy_score = accuracy_score(y_test, predicted_svm)\n",
    "print(SVM_accuracy_score)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_svm))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with half the dataset\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train_half, y_train_half)\n",
    "predicted_svm = svm.predict(X_test_half)\n",
    "print(\"SVM Accuracy:\")\n",
    "SVM_accuracy_score_half = accuracy_score(y_test_half, predicted_svm)\n",
    "print(SVM_accuracy_score_half)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_half, predicted_svm))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test_half, predicted_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "dt = DecisionTreeClassifier(max_depth=10, min_samples_split=25)\n",
    "dt.fit(X_train, y_train)\n",
    "predicted_dt = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\")\n",
    "DT_accuracy_score = accuracy_score(y_test, predicted_dt)\n",
    "print(DT_accuracy_score)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_dt))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees with half the dataset\n",
    "dt = DecisionTreeClassifier(max_depth=10, min_samples_split=25)\n",
    "dt.fit(X_train_half, y_train_half)\n",
    "predicted_dt = dt.predict(X_test_half)\n",
    "print(\"Decision Tree Accuracy:\")\n",
    "DT_accuracy_score_half = accuracy_score(y_test_half, predicted_dt)\n",
    "print(DT_accuracy_score_half)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_half, predicted_dt))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test_half, predicted_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=20)\n",
    "rf.fit(X_train, y_train)\n",
    "predicted_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\")\n",
    "RF_accuracy_score = accuracy_score(y_test, predicted_rf)\n",
    "print(RF_accuracy_score)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_rf))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with half the dataset\n",
    "rf = RandomForestClassifier(n_estimators=20)\n",
    "rf.fit(X_train_half, y_train_half)\n",
    "predicted_rf = rf.predict(X_test_half)\n",
    "print(\"Random Forest Accuracy:\")\n",
    "RF_accuracy_score_half = accuracy_score(y_test_half, predicted_rf)\n",
    "print(RF_accuracy_score_half)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_half, predicted_rf))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test_half, predicted_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "bc = BaggingClassifier()\n",
    "bc.fit(X_train, y_train)\n",
    "predicted_bc = bc.predict(X_test)\n",
    "print(\"Bagging Accuracy:\")\n",
    "Bagging_accuracy_score = accuracy_score(y_test, predicted_bc)\n",
    "print(Bagging_accuracy_score)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_bc))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted_bc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging with half the dataset\n",
    "bc = BaggingClassifier()\n",
    "bc.fit(X_train_half, y_train_half)\n",
    "predicted_bc = bc.predict(X_test_half)\n",
    "print(\"Bagging Accuracy:\")\n",
    "Bagging_accuracy_score_half = accuracy_score(y_test_half, predicted_bc)\n",
    "print(Bagging_accuracy_score_half)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_half, predicted_bc))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test_half, predicted_bc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradiant Boosting\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "predicted_gb = gb.predict(X_test)\n",
    "print(\"Gradiant Boosting Accuracy:\")\n",
    "GB_accuracy_score = accuracy_score(y_test, predicted_gb)\n",
    "print(GB_accuracy_score)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_gb))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradiant Boosting with half the dataset\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train_half, y_train_half)\n",
    "predicted_gb = gb.predict(X_test_half)\n",
    "print(\"Gradiant Boosting Accuracy:\")\n",
    "GB_accuracy_score_half = accuracy_score(y_test_half, predicted_gb)\n",
    "print(GB_accuracy_score_half)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_half, predicted_gb))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test_half, predicted_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = [\n",
    "    'MultinomialNB', \n",
    "    'KNN', \n",
    "    'SVM', \n",
    "    'Decision Tree', \n",
    "    'Random Forest', \n",
    "    'Bagging', \n",
    "    'Gradient Boosting'\n",
    "]\n",
    "\n",
    "# Accuracy scores for each model\n",
    "accuracy_scores = [\n",
    "    MC_accuracy_score, \n",
    "    KNN_accuracy_score, \n",
    "    SVM_accuracy_score, \n",
    "    DT_accuracy_score, \n",
    "    RF_accuracy_score, \n",
    "    Bagging_accuracy_score, \n",
    "    GB_accuracy_score\n",
    "]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(model_names, accuracy_scores, color='skyblue')\n",
    "plt.title('Comparison of Accuracy Scores between Models')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = [\n",
    "    'MultinomialNB', \n",
    "    'KNN', \n",
    "    'SVM', \n",
    "    'Decision Tree', \n",
    "    'Random Forest', \n",
    "    'Bagging', \n",
    "    'Gradient Boosting'\n",
    "]\n",
    "\n",
    "# Accuracy scores for each model\n",
    "accuracy_scores = [\n",
    "    MC_accuracy_score_half, \n",
    "    KNN_accuracy_score_half, \n",
    "    SVM_accuracy_score_half, \n",
    "    DT_accuracy_score_half, \n",
    "    RF_accuracy_score_half, \n",
    "    Bagging_accuracy_score_half, \n",
    "    GB_accuracy_score_half\n",
    "]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(model_names, accuracy_scores, color='skyblue')\n",
    "plt.title('Comparison of Accuracy Scores between Models with half the Dataset')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = [\n",
    "    'MultinomialNB', \n",
    "    'KNN', \n",
    "    'SVM', \n",
    "    'Decision Tree', \n",
    "    'Random Forest', \n",
    "    'Bagging', \n",
    "    'Gradient Boosting'\n",
    "]\n",
    "\n",
    "# Difference in Accuracy scores for each model and datasets\n",
    "accuracy_scores = [\n",
    "    MC_accuracy_score - MC_accuracy_score_half, \n",
    "    KNN_accuracy_score - KNN_accuracy_score_half, \n",
    "    SVM_accuracy_score - SVM_accuracy_score_half, \n",
    "    DT_accuracy_score - DT_accuracy_score_half, \n",
    "    RF_accuracy_score - RF_accuracy_score_half, \n",
    "    Bagging_accuracy_score - Bagging_accuracy_score_half, \n",
    "    GB_accuracy_score - GB_accuracy_score_half\n",
    "]\n",
    "\n",
    "print(accuracy_scores)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(model_names, accuracy_scores, color='skyblue')\n",
    "plt.title('Comparison of Difference in Accuracy Scores between Models over Datasets')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy Score - Accuracy Score over Half')\n",
    "plt.ylim(-0.1, 0.2)\n",
    "plt.xticks(rotation=45)  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation and Hyperparameter Tuning\n",
    "We will implement cross validation and Hyperparameter tuning for selected base models which already show promissing results (hight accuracy and consistent confussion matrix) so:  \n",
    "KNN,  \n",
    "Random Forest,  \n",
    "Bagging  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation implementation and K Hyperparameter tuning for KNN\n",
    "\n",
    "# Create a copy of training data\n",
    "X_CV = X_train.copy()\n",
    "y_CV = y_train.copy()\n",
    "\n",
    "# Define the number of folds\n",
    "folds = 3\n",
    "\n",
    "# Initialize variables for storing evaluation results and overall scores\n",
    "eval_core = []\n",
    "overall_scores = []\n",
    "\n",
    "# We will check K in the range from 1 to 10\n",
    "for neigh in range(1, 11):\n",
    "\n",
    "    # Initialize variables to store fold-specific evaluation metrics\n",
    "    fold_eval = []\n",
    "\n",
    "    # Initialize k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=False)\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X_CV, y_CV)):\n",
    "        # Select data from train and val based on fold indices\n",
    "        X_CV_train, X_CV_val = X_CV[train_index], X_CV[val_index]\n",
    "        y_CV_train, y_CV_val = y_CV.iloc[train_index], y_CV.iloc[val_index]\n",
    "\n",
    "        # Train a KNN classifier with the given parameter k\n",
    "        knn = KNeighborsClassifier(n_neighbors=neigh)\n",
    "        knn.fit(X_CV_train, y_CV_train)\n",
    "\n",
    "        # Predict on the val set\n",
    "        y_CV_pred = knn.predict(X_CV_val)\n",
    "\n",
    "        # Compute the accuracy metric for this fold\n",
    "        accuracy = accuracy_score(y_CV_val, y_CV_pred)\n",
    "        fold_eval.append(accuracy)\n",
    "\n",
    "    # Calculate the mean performance across all folds for this k\n",
    "    mean_accuracy = np.mean(fold_eval)\n",
    "    print(f'n_neighbours_{neigh}:', mean_accuracy)\n",
    "\n",
    "    # Append the mean accuracy to the overall scores\n",
    "    overall_scores.append(mean_accuracy)\n",
    "\n",
    "# Diagnostic\n",
    "print('Overall scores:', overall_scores)\n",
    "\n",
    "# Find the best performing k based on the highest accuracy\n",
    "best_k = np.argmax(overall_scores) + 1  # Add 1 to convert to 1-based index\n",
    "print(\"Best K for KNN:\", best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# accuracy of Knn with diffent K's\n",
    "Ks = (np.arange(0, 10) + 1)\n",
    "\n",
    "# Accuracy scores for each model\n",
    "accuracy_scores = overall_scores\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(Ks, accuracy_scores, color='skyblue')\n",
    "plt.title('Comparison of Accuracy Scores between differnet K')\n",
    "plt.xlabel('Number of K')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Hyperparameter tuning using Grid Search with cross validation throught gridsearch Object\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 150],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 20, 50],  # Adjust the range as needed\n",
    "    'min_samples_split': [10, 30],\n",
    "    'min_samples_leaf': [1, 10],    \n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a Random Forest classifier with the best hyperparameters\n",
    "best_rf = RandomForestClassifier(**best_params)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on your test data\n",
    "predicted_rf = best_rf.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy of the tuned Random Forest\n",
    "accuracy_rf = accuracy_score(y_val, predicted_rf)\n",
    "print(\"Tuned Decision Tree Accuracy:\", accuracy_rf)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "predicted_test = best_rf.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, predicted_test)\n",
    "print(\"Accuracy en conjunto de prueba:\", accuracy_test)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search for Random Forest with cross validation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [None, 20, 50]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=10, cv=3)\n",
    "random_search_rf.fit(X_val, y_val)\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "predicted_best_rf = best_rf.predict(X_test)\n",
    "accuracy_best_rf = accuracy_score(y_test, predicted_best_rf)\n",
    "print(\"Best Random Forest Accuracy:\", accuracy_best_rf)\n",
    "print(random_search_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rf = 0.844\n",
    "results = [RF_accuracy_score, accuracy_rf, accuracy_best_rf]\n",
    "classifiers = ['Base', 'Grid Search', 'Random Search']\n",
    "\n",
    "# Sort results and classifiers in ascending order of accuracy\n",
    "sorted_results, sorted_classifiers = zip(*sorted(zip(results, classifiers)))\n",
    "\n",
    "# Create a bar plot\n",
    "plt.barh(sorted_classifiers, sorted_results, color='skyblue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Random Forest Accuracy Comparison (Low to High)')\n",
    "plt.xlim(0, 1.0)  # Set the x-axis limits from 0 to 1 for accuracy values\n",
    "\n",
    "# Annotate the bars with accuracy values\n",
    "for i, result in enumerate(sorted_results):\n",
    "    plt.text(result + 0.01, i, f'{result:.2f}', va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Bagging using Grid Search with cross validation\n",
    "param_grid = {\n",
    "              'n_estimators': [10, 20, 30],\n",
    "              'max_samples': [0.8, 1, 2],\n",
    "              'max_features': [0.5, 1],\n",
    "              'bootstrap': [True, False],\n",
    "              'bootstrap_features': [True, False],\n",
    "              'warm_start': [True, False]\n",
    "}\n",
    "\n",
    "bc = BaggingClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(bc, param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_bc = grid_search.best_estimator_\n",
    "predicted_best_bc = best_bc.predict(X_val)\n",
    "accuracy_best_bc = accuracy_score(y_val, predicted_best_bc)\n",
    "print(\"Best Bagging Accuracy:\", accuracy_best_bc)\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "predicted_test = best_bc.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, predicted_test)\n",
    "print(\"Accuracy en conjunto de prueba:\", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [Bagging_accuracy_score, accuracy_test]\n",
    "classifiers = ['Base', 'Grid Search']\n",
    "\n",
    "# Sort results and classifiers in ascending order of accuracy\n",
    "sorted_results, sorted_classifiers = zip(*sorted(zip(results, classifiers)))\n",
    "\n",
    "# Create a bar plot\n",
    "plt.barh(sorted_classifiers, sorted_results, color='skyblue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Bagging Accuracy Comparison (Low to High)')\n",
    "plt.xlim(0, 1.0)  # Set the x-axis limits from 0 to 1 for accuracy values\n",
    "\n",
    "# Annotate the bars with accuracy values\n",
    "for i, result in enumerate(sorted_results):\n",
    "    plt.text(result + 0.01, i, f'{result:.2f}', va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"dair-ai/emotion\", \"unsplit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el split 'train' a un DataFrame de Pandas\n",
    "df = pd.DataFrame(ds['train'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution\n",
    "class_distribution = df['label'].value_counts().sort_index()\n",
    "\n",
    "class_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of the Emotion variable')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Number of examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsamplig label 0 and 1\n",
    "df_label_0 = df[df['label'] == 0]\n",
    "df_label_1 = df[df['label'] == 1]\n",
    "df_other_labels = df[df['label'].isin([2, 3, 4, 5])]\n",
    "\n",
    "df_label_0_downsampled = df_label_0.sample(n=60000, random_state=42)\n",
    "df_label_1_downsampled = df_label_1.sample(n=60000, random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_label_0_downsampled, df_label_1_downsampled, df_other_labels], ignore_index=True)\n",
    "\n",
    "print(df_balanced['label'].value_counts())\n",
    "\n",
    "texts = df[\"text\"]\n",
    "labels = df[\"label\"]\n",
    "\n",
    "# Create TF-IDF vectorizer with optimized parameters\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    ngram_range=(1, 3),\n",
    "    strip_accents='unicode',\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Transform texts to TF-IDF features\n",
    "X = tfidf.fit_transform(texts)\n",
    "\n",
    "# Split the data with a smaller test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors = best_k,),\n",
    "    \"rf\": best_rf,\n",
    "    \"bagging\": bc\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(name + \" trained.\")\n",
    "\n",
    "\n",
    "final_predictions = (\n",
    "    0.7 * models['rf'].predict(X_test) +\n",
    "    0.2 * models['bagging'].predict(X_test) +\n",
    "    0.1 * models['knn'].predict(X_test)\n",
    ")\n",
    "final_predictions = (final_predictions > 0.5).astype(int)  # Convert to binary predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Results\n",
    "print(\"\\nENSEMBLE MODEL PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, final_predictions):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks with Keras  \n",
    "\n",
    "RNNs are particularly effective for sequence data, such as text, where the order of words matters. These models process words one by one, maintaining a memory of previous words, which allows them to capture dependencies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/dair-ai/emotion/unsplit/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsamplig label 0 and 1\n",
    "df_label_0 = df[df['label'] == 0]\n",
    "df_label_1 = df[df['label'] == 1]\n",
    "df_other_labels = df[df['label'].isin([2, 3, 4, 5])]\n",
    "\n",
    "df_label_0_downsampled = df_label_0.sample(n=60000, random_state=42)\n",
    "df_label_1_downsampled = df_label_1.sample(n=60000, random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_label_0_downsampled, df_label_1_downsampled, df_other_labels], ignore_index=True)\n",
    "\n",
    "print(df_balanced['label'].value_counts())\n",
    "\n",
    "X = df_balanced[\"text\"]  # Use the 'text' column from df_balanced\n",
    "y = df_balanced[\"label\"]\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=df_balanced[\"label\"], random_state=123)\n",
    "\n",
    "# Sizing\n",
    "print(f\"Training data size: {len(X_train)}\")\n",
    "print(f\"Test data size: {len(X_test)}\")\n",
    "\n",
    "# Tokenization parameters\n",
    "max_words = 20000  # Vocabulary size\n",
    "max_len = 100       # Maximum sequence length\n",
    "\n",
    "# Initialize and fit the tokenizer on the training data\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Training data shape: {X_train_seq.shape}\")\n",
    "print(f\"Test data shape: {X_test_seq.shape}\")\n",
    "print(f\"Type of y_train: {type(y_train.iloc[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(max_len,)),\n",
    "    tf.keras.layers.Embedding(20000, 100, input_length=max_len),  # Increased embedding size\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=False)),  # Increased LSTM size\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Added dense layer\n",
    "    tf.keras.layers.Dense(6, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile with lower learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Early stopping with more patience\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor val_loss instead\n",
    "    patience=5,  # More patience to avoid early stopping too soon\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-7)\n",
    "\n",
    "# Train with slightly different parameters\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test_seq, y_test)\n",
    "print(f\"\\nTest accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are available in the README file: https://github.com/sofiaamartinezz/IS_Seminar2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
